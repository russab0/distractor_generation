# -*- coding: utf-8 -*-
"""BDG - distractor generation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/195gk-NlgMNlmDzZMk5dTJ120XBFQRQTK

# BDG

Source: https://github.com/voidful/BDG && https://github.com/voidful/TFkit/tree/master/tfkit/model/onebyone


Source of the source: https://github.com/alontalmor/pytorch-transformers/blob/master/examples/run_multiple_choice.py

Showcase: https://voidful.tech/DG-Showcase/

### Libraries:

- **TFKit** lets everyone make use of transformer architecture on many tasks and models in small change of config. At the same time, it can do multi-task multi-model learning, and can introduce its own data sets and tasks through simple modifications.
- **NLPrep**: download and preprocessing data in one line
- **nlp2go**: create demo api as quickly as possible.

### Task

All data will be in csv format. No header needed. Each token have to separate by space
#### **onebyone (the same for once)**
[example file](https://github.com/voidful/TFkit/blob/master/demo_data/gen_eng.csv)

Format: input, target

Example: `"i go to school by bus", "我 坐 巴 士 上 學"`

<img src=https://i.ibb.co/QPbjt2X/flow.png width="500">
"""

"""from google.colab import drive
drive.mount('/drive')

PATH = '/drive/My Drive/Thesis/BDG'

!pip install inquirer
!pip install tensorboardX
!pip install nlp2>=1.8.26
!pip install transformers
!pip install jsonlines
!pip install transformers<=4.0.1,>=3.3.0
!pip install torch
!pip install sklearn
#!pip install matplotlib
!pip install tqdm>=4.45.0
#!pip install inquirer
#!pip install numpy
#!pip install pytorch-crf

!nvidia-smi"""

"""## Data preprocessing

### Downloading data
"""

# source https://github.com/Yifan-Gao/Distractor-Generation-RACE

"""!wget https://raw.githubuserconte12284nt.com/Yifan-Gao/Distractor-Generation-RACE/master/data/data.tar.gz
!tar xvzf data.tar.gz

!wget http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz
!tar xvzf RACE.tar.gz

from google.colab import drive
drive.mount('/drive')

!mkdir "processed_data"
!mkdir "/drive/My Drive/Thesis/BDG/processed_data"""""

import sys
import os

PATH = f'/home/vivanov/distractor_generation'
DATA_PATH = f'{PATH}/data/processed'
sys.path.append(PATH)

# !ls '/drive/My Drive/Thesis/BDG'


"""### Dataset"""

import os
import pickle
from random import choice

import nlp2
import numpy
import numpy as np
from torch.utils import data
from tqdm import tqdm
from transformers import AutoTokenizer, BertTokenizer


def check_type_for_dataloader(data_item):
    if (isinstance(data_item, list) and not isinstance(data_item[-1], str) and check_type_for_dataloader(
            data_item[-1])) or \
            isinstance(data_item, str) or \
            isinstance(data_item, numpy.ndarray) or \
            isinstance(data_item, int):
        return True
    else:
        return False


def get_dataset(file_path, model_class, parameter, cut=None, lazy=False):
    """
    parameter: join of model_arg and input_arg
    cut: at each step of the loading to break
    """
    panel = nlp2.Panel()  # what for???
    all_arg = nlp2.function_get_all_arg_with_value(
        preprocessing_data)  # default arguments set in the function declaration
    if parameter.get('panel'):
        for missarg in nlp2.function_check_missing_arg(preprocessing_data,
                                                       parameter):  # for all args that are not in paremeter dict
            panel.add_element(k=missarg, v=all_arg[missarg], msg=missarg, default=all_arg[missarg])
            print('Missarg', missarg, all_arg[missarg])
        filled_arg = panel.get_result_dict()
        parameter.update(filled_arg)

    if lazy:
        D = LazyDataset
    else:
        D = LoadDataset
    print(D)
    ds = D(fpath=file_path,
           pretrained_config=parameter.get('config'),
           get_data_from_file=get_data_from_file,
           preprocessing_data=preprocessing_data,
           cache=parameter.get('cache'),
           input_arg=parameter,
           cut=cut)

    return ds


class LoadDataset(data.Dataset):
    def __init__(self, fpath, pretrained_config, get_data_from_file, preprocessing_data, cache=False, input_arg={},
                 cut=None):
        sample = []
        if 'albert_chinese' in pretrained_config:
            tokenizer = BertTokenizer.from_pretrained(pretrained_config)
        else:
            tokenizer = AutoTokenizer.from_pretrained(pretrained_config)

        cache_path = fpath + "_" + pretrained_config.replace("/", "_") + ".cache"
        print('using cache', os.path.isfile(cache_path) and cache)
        print('cache path', cache_path)

        if os.path.isfile(cache_path) and cache:
            with open(cache_path, "rb") as cf:
                outdata = pickle.load(cf)
                sample = outdata['sample']
                task_dict = outdata['task']
        else:
            task_dict = {}
            total_data = 0
            total_lines = 0
            for i in tqdm(get_data_from_file(fpath),
                          desc='loading data'):  # tasks, task, input, [target, negative_text]
                # i - item
                if cut < total_lines:
                    print(f"Broke after processing {total_lines} lines")
                    break

                tasks, task, input_text, target, *other = i
                task_dict.update(tasks)
                total_lines += 1
                for get_feature_from_data, feature_param in preprocessing_data(i, tokenizer, **input_arg):
                    # print(get_feature_from_data)
                    # print('LoadDataset: feature_param', total_size(feature_param))
                    for feature in get_feature_from_data(**feature_param):
                        feature = {k: v for k, v in feature.items() if check_type_for_dataloader(
                            v)}  # filter out (k, v) pairs if they are not fittable into dataloader
                        # print('LoadDataset: feature', total_size(feature))
                        # print(feature)
                        sample.append(feature)
                        total_data += 1

                # break # TODO remove
            print("Processed " + str(total_data) + " data.")

            if cache:
                with open(cache_path, 'wb') as cf:
                    outdata = {'sample': sample, 'task': task_dict}
                    pickle.dump(outdata, cf)

        self.sample = sample
        self.task = task_dict

    def increase_with_sampling(self, total):
        inc_samp = [choice(self.sample) for _ in range(total - len(self.sample))]
        self.sample.extend(inc_samp)

    def __len__(self):
        return len(self.sample)

    def __getitem__(self, idx):
        # b = self.sample[idx]
        # print(type(self.sample[idx]))
        # ar = [len(b['target']), len(b['ntarget']), len(b['type']), len(b['mask'])]
        # if 513 in ar or 514 in ar:
        #    print(ar, b)

        self.sample[idx].update((k, np.asarray(v)) for k, v in self.sample[idx].items() if k != 'task')
        return self.sample[idx]


class LazyDataset(data.Dataset):
    def __init__(self, fpath, pretrained_config, get_data_from_file, preprocessing_data, cache=False, input_arg={},
                 cut=None):
        self.task = {'default': []}
        self.sample = []
        if 'albert_chinese' in pretrained_config:
            self.tokenizer = BertTokenizer.from_pretrained(pretrained_config)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(pretrained_config)
        self.fpath = fpath
        self.file = get_full_file(fpath)
        self.pretrained_config = pretrained_config
        self.cache = cache
        self.input_arg = input_arg
        self.cut = cut

    def increase_with_sampling(self, total):
        inc_samp = [choice(self.sample) for _ in range(total - len(self.sample))]
        self.sample.extend(inc_samp)

    def __len__(self):
        return len(self.file)

    def __getitem__(self, idx):
        tokenizer = self.tokenizer

        cache_path = self.fpath + "_" + pretrained_config.replace("/", "_") + ".cache"
        if False and os.path.isfile(cache_path) and self.cache:
            with open(cache_path, "rb") as cf:
                outdata = pickle.load(cf)
                sample = outdata['sample']
                task_dict = outdata['task']
        else:
            task_dict = {}
            i = get_data_from_row(self.file[idx])

            tasks, task, input_text, target, *other = i
            task_dict.update(tasks)
            for get_feature_from_data, feature_param in preprocessing_data(i, tokenizer, **input_arg):
                for feature in get_feature_from_data(**feature_param):
                    feature = {k: v for k, v in feature.items() if check_type_for_dataloader(
                        v)}  # filter out (k, v) pairs if they are not fittable into dataloader
                    self.sample.append(feature)

            """if self.cache:
                with open(cache_path, 'wb') as cf:
                    outdata = {'sample': self.sample, 'task': self.task_dict}
                    pickle.dump(outdata, cf)"""

        # self.sample = sample
        # self.task = task_dict

        feature.update((k, np.asarray(v)) for k, v in feature.items() if k != 'task')
        return feature


"""### Logger"""

import csv
import os
import tensorboardX as tb


class Logger:
    def __init__(self, savedir, logfilename="message.log", metricfilename="metric.log", tensorboard=False):
        self.savedir = savedir
        self.logfilepath = os.path.join(savedir, logfilename)
        self.metricfilepath = os.path.join(savedir, metricfilename)
        self.tensorboard_writer = tb.SummaryWriter() if tensorboard else None

    def write_log(self, *args):
        """
        Write a line of args
        """
        line = ' '.join([str(a) for a in args])  # concatenate args
        with open(self.logfilepath, "a", encoding='utf8') as log_file:
            log_file.write(line + '\n')
        print(line)

    def write_metric(self, *args):
        if self.tensorboard_writer:
            self.tensorboard_writer.add_scalar(*args)
        else:
            with open(self.metricfilepath, "a", encoding='utf8') as log_file:
                writer = csv.writer(log_file)
                writer.writerow(args)


#

"""### Model loader"""

import importlib
import os

import inquirer
import nlp2
import torch
from transformers import BertTokenizer, AutoTokenizer, AutoModel


def list_all_model(ignore_list=[]):
    return ['onebyone']
    # dataset_dir = os.path.abspath(__file__ + "/../../") + '/model'
    # return list(filter(
    #    lambda x: os.path.isdir(os.path.join(dataset_dir, x)) and '__pycache__' not in x and x not in ignore_list,
    #    os.listdir(dataset_dir)))


def load_predict_parameter(model, enable_arg_panel=False):
    """use inquirer panel to let user input model parameter or just use default value"""
    return nlp2.function_argument_panel(model.predict, disable_input_panel=(not enable_arg_panel), func_parent=model,
                                        ignore_empty=True)


def load_model_class(model_name):
    return importlib.import_module('.' + model_name, 'model')


def get_model_class():
    try:
        name = input_arg.get('setting')
    except NameError:
        name = eval_arg.get('setting')
    if name in MODEL_CONFIG_MAPPING:
        return MODEL_CONFIG_MAPPING[name]
    raise NotImplemented('Unknown model name config')


def load_trained_model(model_path, pretrained_config=None, model_type=None):
    """loading saved model"""

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    torchpack = torch.load(model_path, map_location=device)

    print("===model info===")
    [print(key, ':', torchpack[key]) for key in torchpack.keys() if 'state_dict' not in key and 'models' not in key]
    print('==========')

    if 'tags' in torchpack and len(torchpack['tags']) > 1:
        if model_type is None:
            print("Pick which models to use in multi-task models")
            inquirer_res = inquirer.prompt(
                [inquirer.List('model_type', message="Select model", choices=torchpack['tags'])])
            model_type = inquirer_res['model_type']
        type_ind = torchpack['tags'].index(model_type)
    else:
        type_ind = 0

    print("loading saved model")

    # get all loading parameter
    maxlen = torchpack['maxlen']
    if pretrained_config is not None:
        config = pretrained_config
    else:
        config = torchpack['model_config'] if 'model_config' in torchpack else torchpack['bert']
    model_types = [torchpack['type']] if not isinstance(torchpack['type'], list) else torchpack['type']
    models_state = torchpack['models'] if 'models' in torchpack else [torchpack['model_state_dict']]
    type = model_types[type_ind]

    # load model
    if 'albert_chinese' in config:
        tokenizer = BertTokenizer.from_pretrained(config)
    else:
        tokenizer = AutoTokenizer.from_pretrained(config)
    pretrained = AutoModel.from_pretrained(config)

    if 'tag' in type:  # for old version model
        type = 'tag'
    elif 'onebyone' in type:
        type = 'onebyone'

    # model_class = Model
    model_class = get_model_class()
    task_detail = {}
    if 'task-label' in torchpack:
        task_detail = torchpack['task-label']
    elif 'label' in torchpack:
        task_detail = {'label': torchpack['label']}

    model = model_class(tokenizer=tokenizer, pretrained=pretrained, tasks_detail=task_detail,
                        maxlen=maxlen)
    model = model.to(device)
    model.load_state_dict(models_state[type_ind], strict=False)

    print("finish loading")
    return model, type, model_class


"""### Loss"""

import torch
from torch import nn
import torch.nn.functional as F
from torch.autograd import Variable


class NegativeCElLoss(nn.Module):
    def __init__(self, ignore_index=-1, reduction='mean'):
        super(NegativeCElLoss, self).__init__()
        self.softmax = nn.Softmax(dim=1)
        self.alpha = 1
        self.nll = nn.NLLLoss(ignore_index=ignore_index, reduction=reduction)

    def forward(self, input, target):
        nsoftmax = self.softmax(input)
        nsoftmax = torch.clamp((1.0 - nsoftmax), min=1e-32)
        return self.nll(torch.log(nsoftmax) * self.alpha, target)


"""### Tok"""

from collections import OrderedDict

import nlp2
from tqdm import tqdm


def tok_begin(tokenizer):
    if isinstance(tokenizer.cls_token, str):
        return tokenizer.cls_token
    elif isinstance(tokenizer.bos_token, str):
        return tokenizer.bos_token
    return 'cls'


def tok_sep(tokenizer):
    if isinstance(tokenizer.sep_token, str):
        return tokenizer.sep_token
    elif isinstance(tokenizer.eos_token, str):
        return tokenizer.eos_token
    return 'sep'


def tok_mask(tokenizer):
    if isinstance(tokenizer.mask_token, str):
        return tokenizer.mask_token
    return 'msk'


def tok_pad(tokenizer):
    if isinstance(tokenizer.pad_token, str):
        return tokenizer.pad_token
    return 'pad'


def handle_exceed(tokenizer, seq, maxlen, mode=['noop', 'remove', 'slide', 'start_slice', 'end_slice'],
                  keep_after_sep=True):
    mode = mode[0] if isinstance(mode, list) else mode
    seq = seq.replace("[MASK]", tok_mask(tokenizer)).replace("[SEP]", tok_sep(tokenizer)).replace("[CLS]",
                                                                                                  tok_begin(tokenizer))
    sep_split = seq.split(tok_sep(tokenizer))
    ext_seq = [tok_sep(tokenizer)] + tokenizer.tokenize(tok_sep(tokenizer).join(sep_split[1:])) \
        if len(sep_split) > 1 and keep_after_sep else []
    t_seq = tokenizer.tokenize(sep_split[0])
    if mode == 'noop':
        return [t_seq + ext_seq], [[0, len(t_seq + ext_seq)]]
    if mode == 'remove':
        if len(t_seq + ext_seq) <= maxlen:
            return [t_seq + ext_seq], [[0, len(t_seq + ext_seq)]]
        else:
            return [], [[0, 0]]
    if mode == 'slide':
        return nlp2.sliding_windows(t_seq, maxlen - len(ext_seq), append_seq=ext_seq)
    if mode == 'start_slice':
        slices = t_seq[:maxlen - len(ext_seq)]
        slices.extend(ext_seq)
        return [slices], [[0, maxlen - len(ext_seq)]]
    if mode == 'end_slice':
        start_pos = len(t_seq) + len(ext_seq) - maxlen
        slices = t_seq[start_pos:]
        slices.extend(ext_seq)
        return [slices], [[max(0, start_pos), len(t_seq)]]


def get_topP_unk_token(tokenizer, file_paths: list, topP: float):
    unk_count_dict = OrderedDict()
    for path in file_paths:
        for input_sent in tqdm(nlp2.read_files_yield_lines(path)):
            for tok in nlp2.split_sentence_to_array(input_sent):
                if tokenizer._unk_token in tokenizer.tokenize(tok):
                    unk_count_dict[tok] = unk_count_dict.get(tok, 0) + 1
    top_range = int((len(unk_count_dict) + 1) * topP * 100)
    return list(unk_count_dict.keys())[:top_range]


def get_freqK_unk_token(tokenizer, file_paths: list, freqK: int):
    unk_count_dict = OrderedDict()
    for path in file_paths:
        for input_sent in tqdm(nlp2.read_files_yield_lines(path)):
            for tok in nlp2.split_sentence_to_array(input_sent):
                if tokenizer._unk_token in tokenizer.tokenize(tok):
                    unk_count_dict[tok] = unk_count_dict.get(tok, 0) + 1
    return [key for key, value in unk_count_dict.items() if value >= freqK]


"""### Eval_metric"""

from collections import defaultdict
import string
import re
from collections import Counter


def _normalize_answer(s):
    """Lower text and remove punctuation, articles and extra whitespace."""

    def remove_articles(text):
        if len(text) > 1:
            return re.sub(r'\b(a|an|the)\b', ' ', text)
        else:
            return text

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))


def _f1_score(prediction, ground_truth):
    prediction_tokens = _normalize_answer(prediction).split()
    ground_truth_tokens = _normalize_answer(ground_truth).split()
    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1


class EvalMetric:

    def __init__(self, tokenizer, max_candidate=6):
        self.tasks = defaultdict(lambda: defaultdict(list))
        self.max_candidate = max_candidate
        self.tokenizer = tokenizer
        self.target_list = defaultdict(lambda: defaultdict(int))

    def tokenize_text(self, text):
        return self.tokenizer.convert_tokens_to_string(self.tokenizer.tokenize(text))

    def add_record(self, input, predicted, target, task='default'):
        if isinstance(input, str):
            input = self.tokenize_text(input.strip())
        if isinstance(input, list):
            for i, t in enumerate(input):
                input[i] = self.tokenize_text(t.strip())

        if isinstance(predicted, str):
            predicted = self.tokenize_text(predicted)
        if isinstance(predicted, list):
            for i, t in enumerate(predicted):
                predicted[i] = self.tokenize_text(t.strip())

        if isinstance(target, str):
            targets = []
            if "[SEP]" in target:
                targets.extend([self.tokenize_text(st.strip()) for st in target.split("[SEP]")])
            else:
                targets.append(self.tokenize_text(target.strip()))
        if isinstance(target, list):
            for i, t in enumerate(target):
                target[i] = self.tokenize_text(t.strip())
            targets = target

        if self.max_candidate - len(targets) > 0 and "nlg" in task:
            targets.extend([""] * (self.max_candidate - len(targets)))

        for t in targets:
            self.target_list[task][t] += 1

        self.tasks[task]['input'].append(input)
        self.tasks[task]['predicted'].append(predicted)
        self.tasks[task]['predicteds'].append([predicted])
        self.tasks[task]['target'].append(target)
        self.tasks[task]['targets'].append(targets)

    def get_record(self, task='default'):
        return self.tasks[task]

    def cal_score(self, metric):
        data_score = []
        for task_name, task in self.tasks.items():
            print("Task : " + task_name + " report ")
            if "emf1" in metric:
                em = 0
                total = 0
                f1 = 0
                for pos, predict in enumerate(task['predicted']):
                    em_list = []
                    f1_list = []
                    for target in task['targets'][pos]:
                        if _normalize_answer(str(predict)) == _normalize_answer(str(target)) and len(
                                _normalize_answer(str(predict))) > 0 or len(str(predict)) == len(str(target)) == 0:
                            em_score = 1
                            f1_score = 1
                        else:
                            em_score = 0
                            f1_score = _f1_score(str(predict), str(target))
                        em_list.append(em_score)
                        f1_list.append(f1_score)
                    em += max(em_list)
                    f1 += max(f1_list)
                    data_score.append([predict, task['targets'][pos][em_list.index(max(em_list))],
                                       {'em': max(em_list), 'f1': max(f1_list)}])
                    total += 1
                result = {"EM": em / (total or not total), "F1": f1 / (total or not total)}
                data_score = sorted(data_score, key=lambda i: i[2]['em'], reverse=True)
            if "nlg" in metric:
                try:
                    from nlgeval import NLGEval
                except ImportError:
                    print(
                        "nlg-eval package not install, plz install it: pip install git+https://github.com/voidful/nlg-eval.git ; nlg-eval --setup ./nlg-eval-data/")
                    raise
                nlgeval = NLGEval(no_skipthoughts=True, no_glove=True, metrics_to_omit=["METEOR"])
                targets = task['targets']
                predicted = task['predicted']
                for t, p in zip(targets, predicted):
                    data_score.append([p, t, nlgeval.compute_metrics(ref_list=list(map(list, zip(t))), hyp_list=[p])])
                result = nlgeval.compute_metrics(ref_list=list(map(list, zip(*task['targets']))),  # transpose
                                                 hyp_list=predicted)
                data_score = sorted(data_score, key=lambda i: i[2]['ROUGE_L'])
            if "clas" in metric:
                from sklearn.metrics import classification_report
                from sklearn.preprocessing import MultiLabelBinarizer
                from sklearn.metrics import precision_recall_fscore_support
                target_key = [t for t in self.target_list[task_name].keys() if len(t) > 0]
                mlb = MultiLabelBinarizer().fit([target_key])
                # remove all blank target
                task['targets'] = [[j for j in sub if len(j) > 0] for sub in task['targets']]
                # modify for tagging result
                if isinstance(task['predicteds'][0][0], list):
                    task['targets'] = sum([[[j] for j in sub] for sub in task['targets']], [])
                    task['predicteds'] = sum([[[j] for j in sub] for sub in task['predicted']], [])
                    if len(task['targets']) != len(task['predicteds']):
                        diff = len(task['targets']) - len(task['predicteds'])
                        task['predicteds'].extend([['']] * diff)
                targets = task['targets']
                predicted = task['predicteds']
                for p, t in zip(predicted, targets):
                    score = dict(zip(["precision", "recall", "fbeta_score", "support"],
                                     precision_recall_fscore_support(mlb.transform([t]), mlb.transform([p]),
                                                                     average='weighted')))
                    data_score.append([p, t, score])
                print(mlb.classes_)
                result = classification_report(
                    mlb.transform(targets),
                    mlb.transform(predicted),
                    target_names=list(mlb.classes_))
                data_score = sorted(data_score, key=lambda i: i[2]['fbeta_score'])
            yield (task_name, result, data_score)


"""## Model"""


def once_get_feature_from_data(tokenizer, maxlen, input, target=None, ntarget=None, reserved_len=0,
                               handle_exceed_='start_slice', add_end_tok=True, **kwargs):
    feature_dict_list = []
    tokenized_target = tokenizer.tokenize(target) if target is not None else []
    print()
    t_input_list, _ = handle_exceed(tokenizer, input, maxlen - 3 - len(tokenized_target), handle_exceed_)
    for t_input in t_input_list:  # -2 for cls and sep and prediction end sep
        row_dict = dict()
        tokenized_input = [tok_begin(tokenizer)] + t_input[:maxlen - reserved_len - 3] + [tok_sep(tokenizer)]
        mask_id = [1] * len(tokenized_input)
        type_id = [0] * len(tokenized_input)

        row_dict['target'] = [-1] * maxlen
        row_dict['ntarget'] = [-1] * maxlen

        tokenized_input_id = tokenizer.convert_tokens_to_ids(tokenized_input)
        target_start = len(tokenized_input_id)
        target_end = maxlen

        if target is not None:
            if add_end_tok:
                tokenized_target += [tok_sep(tokenizer)]
            tokenized_target_id = [-1] * len(tokenized_input)
            tokenized_target_id.extend(tokenizer.convert_tokens_to_ids(tokenized_target))
            target_end = len(tokenized_target_id) - 1
            tokenized_target_id.extend([-1] * (maxlen - len(tokenized_target_id)))
            row_dict['target'] = tokenized_target_id

        if ntarget is not None:
            tokenized_ntarget = tokenizer.tokenize(ntarget)
            tokenized_ntarget_id = [-1] * target_start
            tokenized_ntarget_id.extend(tokenizer.convert_tokens_to_ids(tokenized_ntarget))
            tokenized_ntarget_id.extend([-1] * (maxlen - len(tokenized_ntarget_id)))
            if len(tokenized_ntarget_id) <= maxlen:
                row_dict['ntarget'] = tokenized_ntarget_id

        tokenized_input_id.extend([tokenizer.mask_token_id] * (maxlen - len(tokenized_input_id)))
        mask_id.extend([0] * (maxlen - len(mask_id)))
        type_id.extend([1] * (maxlen - len(type_id)))

        row_dict['input'] = tokenized_input_id
        row_dict['type'] = type_id
        row_dict['mask'] = mask_id
        row_dict['start'] = target_start
        row_dict['end'] = target_end
        feature_dict_list.append(row_dict)
    return feature_dict_list


import csv
from collections import defaultdict
from tqdm import tqdm


# import src.utility.tok as tok
# import src.model.once as once


def get_data_from_row(i):
    # print(i)
    tasks = defaultdict(list)
    task = 'default'
    tasks[task] = []
    source_text = i[0]
    target_text = i[1].strip().split(" ")
    negative_text = i[2].strip() if len(i) > 2 else None
    input = source_text
    target = target_text
    return tasks, task, input, [target, negative_text]
    # tasks == {'default': []})
    # task == 'default'
    # input - CAQ
    # target - D (distractor)
    # negative_text - A (answer)


def get_data_from_file(fpath):
    with open(fpath, encoding='utf') as csvfile:
        for i in tqdm(list(csv.reader(csvfile))):
            yield get_data_from_row(i)


def get_full_file(fpath):
    tasks = defaultdict(list)
    task = 'default'
    tasks[task] = []
    with open(fpath, encoding='utf') as csvfile:
        return list(csv.reader(csvfile))


def preprocessing_data(item, tokenizer, maxlen=512, handle_exceed='start_slice',
                       likelihood=['none', 'pos', 'neg', 'both'], reserved_len=0, **kwargs):
    """
    item: tuple returned by get_data_from_file()
    tokenizer: tokenizer object
    maxlen:
    handle_exceed:
    likelyhood: model specification
    reserved_len:
    kwargs: cli args
    """

    likelihood = likelihood[0] if isinstance(likelihood, list) else likelihood
    tasks, task, input, targets = item
    p_target, n_target = targets
    input = input.strip()
    tokenized_target = tokenizer.tokenize(" ".join(p_target))
    param_dict = {'tokenizer': tokenizer, 'maxlen': maxlen,
                  'handle_exceed': handle_exceed, 'reserved_len': reserved_len}
    # print(kwargs)
    if kwargs.get('setting') == 'QA_pretrained_triplet':
        param_dict.update({
            'CQA': 'input'
        })

    # each word in sentence
    for j in range(1, len(tokenized_target) + 1):
        if "neg" in likelihood or 'both' in likelihood:
            # formatting neg data in csv
            if n_target is None:
                ntext_arr = [tokenizer.convert_tokens_to_string(tokenized_target[:j - 1])]
            elif "[SEP]" in n_target:
                ntext_arr = [ntext.strip() for ntext in n_target.split("[SEP]")]
            else:
                ntext_arr = [n_target.strip()]
            # adding neg data
            for neg_text in ntext_arr:
                yield get_feature_from_data, {
                    **{'input': input, 'previous': tokenized_target[:j - 1],
                       'target': tokenized_target[:j], 'ntarget': neg_text, "add_end_tok": False},
                    **param_dict}
        else:
            yield get_feature_from_data, {**{'input': input, 'previous': tokenized_target[:j - 1],
                                             'target': tokenized_target[:j], 'ntarget': None}, **param_dict}

    # end of the last word
    if "neg" in likelihood or 'both' in likelihood:
        # formatting neg data in csv
        if n_target is None:
            ntext_arr = [tokenizer.convert_tokens_to_string(tokenized_target[:j])]
        elif "[SEP]" in n_target:
            ntext_arr = [ntext.strip() for ntext in n_target.split("[SEP]")]
        else:
            ntext_arr = [n_target.strip()]
        # adding neg data
        for neg_text in ntext_arr:
            yield get_feature_from_data, {**{'input': input, 'previous': tokenized_target,
                                             'target': [tok_sep(tokenizer)], 'ntarget': neg_text}, **param_dict}
    else:
        yield get_feature_from_data, {**{'input': input, 'previous': tokenized_target,
                                         'target': [tok_sep(tokenizer)], 'ntarget': None}, **param_dict}

    # whole sentence masking
    if 'pos' in likelihood:
        yield once_get_feature_from_data, {**{'input': input, 'target': " ".join(p_target)}, **param_dict}
    elif 'both' in likelihood:
        # formatting neg data in csv
        if n_target is None:
            ntext_arr = []
        elif "[SEP]" in n_target:
            ntext_arr = [ntext.strip() for ntext in n_target.split("[SEP]")]
        else:
            ntext_arr = [n_target.strip()]
        for neg_text in ntext_arr:
            yield once_get_feature_from_data, {**{'input': input, 'target': " ".join(p_target), 'ntarget': neg_text},
                                               **param_dict}

    return get_feature_from_data, param_dict


def get_feature_from_data(tokenizer, maxlen, input, previous, target=None, ntarget=None, reserved_len=0,
                          handle_exceed_='start_slice', **kwargs):  # TODO was noop
    feature_dict_list = []
    t_input_list, _ = handle_exceed(tokenizer, input, maxlen - 2 - len(previous) - 1,
                                    handle_exceed_)  # -2 for cls and sep
    for t_input in t_input_list:
        row_dict = dict()
        t_input = [tok_begin(tokenizer)] + \
                  t_input[:maxlen - reserved_len - 2] + \
                  [tok_sep(tokenizer)]
        t_input.extend(previous)
        t_input.append(tok_mask(tokenizer))
        t_input_id = tokenizer.convert_tokens_to_ids(t_input)
        mask_id = [1] * len(t_input)
        target_start = len(t_input_id) - 1
        target_end = maxlen
        t_input_id.extend([0] * (maxlen - len(t_input_id)))
        row_dict['target'] = [-1] * maxlen
        row_dict['ntarget'] = [-1] * maxlen
        tokenized_target_id = None
        if target is not None:
            tokenized_target_id = [-1] * target_start
            tokenized_target_id.append(tokenizer.convert_tokens_to_ids(target)[-1])
            target_end = len(tokenized_target_id) - 1
            tokenized_target_id.extend([-1] * (maxlen - len(tokenized_target_id)))
            row_dict['target'] = tokenized_target_id
        if ntarget is not None and len(tokenizer.tokenize(ntarget)) > 0:
            tokenized_ntarget = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(ntarget))
            tokenized_ntarget_id = [-1] * target_start
            tokenized_ntarget_id.extend(tokenized_ntarget)
            tokenized_ntarget_id.extend([-1] * (maxlen - len(tokenized_ntarget_id)))
            if len(tokenized_ntarget_id) <= maxlen:
                row_dict['ntarget'] = tokenized_ntarget_id

        mask_id.extend([0] * (maxlen - len(mask_id)))
        type_id = [0] * len(t_input)
        type_id.extend([1] * (maxlen - len(type_id)))
        row_dict['input'] = t_input_id
        row_dict['type'] = type_id
        row_dict['mask'] = mask_id
        row_dict['start'] = target_start
        row_dict['end'] = target_end
        # ntarget - negative target
        # target - label
        feature_dict_list.append(row_dict)

    return feature_dict_list


"""### Standard model"""

import json
import sys
import os

# dir_path = os.path.dirname(os.path.realpath(__file__))
# sys.path.append(os.path.abspath(os.path.join(dir_path, os.pardir)))

import torch
from torch import nn
from itertools import combinations
from torch.nn.functional import softmax
from math import log
import numpy as np


# from src.model.onebyone.dataloader import get_feature_from_data
# import src.utility.tok as tok
# from src.utility.loss import NegativeCElLoss


class Model(nn.Module):
    def __init__(self, tokenizer, pretrained, maxlen=512, **kwargs):
        super().__init__()
        self.tokenizer = tokenizer
        self.pretrained = pretrained
        self.model = nn.Linear(self.pretrained.config.hidden_size,
                               self.tokenizer.__len__())  # from the hidden size to the tokens count
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.maxlen = maxlen
        print('Using device:', self.device)
        self.model.to(self.device)

    def forward(self, batch_data, eval=False):
        inputs = batch_data['input']
        targets = batch_data['target']
        negative_targets = batch_data['ntarget']
        masks = batch_data['mask']

        tokens_tensor = torch.as_tensor(inputs).to(self.device)
        mask_tensors = torch.as_tensor(masks).to(self.device)

        outputs = self.pretrained(tokens_tensor, attention_mask=mask_tensors)
        prediction_scores = self.model(outputs[0])  # == self.model(outputs.last_hidden_state)

        if eval:
            result_dict = {
                'label_prob_all': [],
                'label_map': [],
                'prob_list': []
            }
            start = batch_data['start'][0]
            topK = torch.topk(softmax(prediction_scores[0][start], dim=0), 50)
            logit_prob = softmax(prediction_scores[0][start], dim=0).data.tolist()
            prob_result = [(self.tokenizer.convert_ids_to_tokens(id), prob) for prob, id in
                           zip(topK.values.data.tolist(), topK.indices.data.tolist())]

            """try this:
            logit_prob = softmax(prediction_scores[0][start], dim=0)
            logit_prob = softmax(prediction_scores[0][start], dim=0).data.tolist()	            topK = torch.topk(logit_prob, 50)
            prob_result = [(self.tokenizer.convert_ids_to_tokens(id), prob) for prob, id in	            prob_result = [(self.tokenizer.convert_ids_to_tokens(id), prob) for prob, id in
                           zip(topK.values.data.tolist(), topK.indices.data.tolist())]	                           zip(topK.values.data.tolist(), topK.indices.data.tolist())]
            result_dict['prob_list'].append(logit_prob)	            result_dict['prob_list'].append(logit_prob.data.tolist())
            """
            result_dict['prob_list'].append(logit_prob)
            result_dict['label_prob_all'].append(prob_result)
            result_dict['label_map'].append(prob_result[0])
            outputs = result_dict
        else:  # if train
            loss_tensors = torch.as_tensor(targets).to(self.device)
            negativeloss_tensors = torch.as_tensor(negative_targets).to(self.device)
            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)  # -1 index = padding token
            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.pretrained.config.vocab_size),
                                      loss_tensors.view(-1))
            negative_loss_fct = NegativeCElLoss(ignore_index=-1).to(self.device)
            negative_loss = negative_loss_fct(prediction_scores.view(-1, self.pretrained.config.vocab_size),
                                              negativeloss_tensors.view(-1))
            masked_lm_loss += negative_loss
            outputs = masked_lm_loss
        return outputs

    def _jaccard_similarity(self, list1, list2):
        s1 = set(list1)
        s2 = set(list2)
        return len(s1.intersection(s2)) / len(s1.union(s2))

    def _isSimilar(self, s, t):
        return self._jaccard_similarity(s, t) > 0.5

    def _filterSimilar(self, d, topP):
        while True:
            filteredOne = False
            for s, t in combinations(d, 2):
                if self._isSimilar(s[0], t[0]) and len(d) - 1 >= topP:
                    d.remove(t)
                    filteredOne = True
                    break
            if not filteredOne:
                break

    def predict(self, input='', topK=1, topP=0.85, mode=['greedy', 'topK', 'topP'], decodenum=1, filtersim=True,
                reserved_len=0, task=None, handle_exceed='noop'):
        filtersim = json.loads(str(filtersim).lower())
        topK = int(topK)
        topP = float(topP)
        decodenum = int(decodenum)
        mode = mode[0] if isinstance(mode, list) else mode.lower()

        self.eval()
        sequences = [[[], 1.0]]
        with torch.no_grad():
            while True:
                all_candidates = list()
                exceed = False
                for seq in sequences:
                    if tok_sep(self.tokenizer) not in seq[0]:
                        tokens, score = seq

                        feature_dict = get_feature_from_data(self.tokenizer, self.maxlen, input, tokens,
                                                             reserved_len=reserved_len,
                                                             handle_exceed=handle_exceed)[-1]
                        # check input exceed
                        if len(tokens) >= self.maxlen or feature_dict['start'] >= self.maxlen:
                            exceed = True
                            all_candidates.append(seq)
                            continue

                        for k, v in feature_dict.items():
                            feature_dict[k] = [v]
                        predictions = self.forward(feature_dict, eval=True)
                        token_prob_list = predictions['label_prob_all'][0]
                        # topK topP
                        if 'top' in mode:
                            prob_list = [prob for _, prob in token_prob_list]
                            if 'topk' in mode:
                                sample_list = prob_list[:topK]
                                decode_range = max(decodenum, topK)
                                prob_norm = [float(i) / sum(sample_list) for i in sample_list]
                                choice_list = np.random.choice(sample_list, p=prob_norm,
                                                               size=decode_range,
                                                               replace=False)
                            else:
                                topP_list = np.cumsum(prob_list)
                                index_overP = [i for i, x in enumerate(topP_list) if x > topP]
                                index_overP = 0 if len(index_overP) < 1 else index_overP[0]
                                sample_list = prob_list[:index_overP + 1]
                                prob_norm = [float(i) / sum(sample_list) for i in sample_list]
                                choice_list = np.random.choice(sample_list, p=prob_norm,
                                                               size=decodenum)
                            for idx in range(decodenum):
                                sampling_index = prob_list.index(choice_list[idx])
                                k, v = token_prob_list[sampling_index]
                                candidate = [tokens + [k], score + -log(v)]
                                all_candidates.append(candidate)

                        # greedy / beam search
                        else:
                            for k, v in token_prob_list[:50]:
                                if len(tokens) > 0 and tokens[-1] == k or len(k) < 1:
                                    continue
                                candidate = [tokens + [k], score + -log(v)]
                                all_candidates.append(candidate)
                    else:
                        all_candidates.append(seq)

                ordered = sorted(all_candidates, key=lambda tup: tup[1])
                if filtersim:
                    self._filterSimilar(ordered, decodenum)
                sequences = ordered[:decodenum]
                stop = 0
                for i in sequences:
                    # i[0] - sequence,i[1] - sequence score
                    if tok_sep(self.tokenizer) in i[0] or i[1] >= self.maxlen:
                        stop += 1
                if stop == len(sequences) or exceed:
                    break

            for i in range(len(sequences)):
                if tok_sep(self.tokenizer) in sequences[i][0]:  # remove sep token
                    sequences[i][0] = sequences[i][0][:sequences[i][0].index(tok_sep(self.tokenizer))]
                sequences[i][0] = "".join(self.tokenizer.convert_tokens_to_string(sequences[i][0]))
            result_dict = {
                'label_map': sequences
            }
            return [i[0] for i in sequences], [result_dict]


"""### QA_pretrained_triplet"""

from typing import List
from transformers import PreTrainedTokenizer

label_map = {label: i
             for i, label in enumerate(["A", "B", "C", "D"])}
LABELS = ["0", "1", "2", "3"]

"""#### from BDG repo"""


class InputExample(object):
    """A single training/test example for multiple choice"""

    def __init__(self, example_id, question, contexts, endings, label=None):
        """Constructs a InputExample.
        Args:
            example_id: Unique id for the example.
            contexts: list of str. The untokenized text of the first sequence (context of corresponding question).
            question: string. The untokenized text of the second sequence (question).
            endings: list of str. multiple choice's options. Its length must be equal to contexts' length.
            label: (Optional) string. The label of the example. This should be
            specified for train and dev examples, but not for test examples.
        """
        self.example_id = example_id
        self.question = question
        self.contexts = contexts
        self.endings = endings
        self.label = label


class InputFeatures(object):
    def __init__(self, example_id, choices_features, label):
        self.example_id = example_id
        self.choices_features = [
            {"input_ids": input_ids, "input_mask": input_mask, "segment_ids": segment_ids}
            for input_ids, input_mask, segment_ids in choices_features
        ]
        self.label = label


def convert_examples_to_features(
        examples: List[InputExample],
        label_list: List[str],
        max_length: int,
        tokenizer: PreTrainedTokenizer,
        pad_token_segment_id=0,
        pad_on_left=False,
        pad_token=0,
        mask_padding_with_zero=True,
        log=False
) -> List[InputFeatures]:
    """
    Loads a data file into a list of `InputFeatures`
    """

    label_map = {label: i for i, label in enumerate(label_list)}

    features = []
    for (ex_index, example) in tqdm(enumerate(examples), desc="convert examples to features"):
        if log and ex_index % 10000 == 0:
            logger.info("Writing example %d of %d" % (ex_index, len(examples)))
        choices_features = []
        for ending_idx, (context, ending) in enumerate(zip(example.contexts, example.endings)):
            text_a = context
            if example.question.find("_") != -1:
                # this is for cloze question
                text_b = example.question.replace("_", ending)
            else:
                text_b = example.question + " " + ending

            inputs = tokenizer.encode_plus(
                text_a, text_b, add_special_tokens=True, max_length=max_length, return_token_type_ids=True
            )
            if "num_truncated_tokens" in inputs and inputs["num_truncated_tokens"] > 0:
                logger.info(
                    "Attention! you are cropping tokens (swag task is ok). "
                    "If you are training ARC and RACE and you are poping question + options,"
                    "you need to try to use a bigger max seq length!"
                )

            input_ids, token_type_ids = inputs["input_ids"], inputs["token_type_ids"]

            # The mask has 1 for real tokens and 0 for padding tokens. Only real
            # tokens are attended to.
            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)

            # Zero-pad up to the sequence length.
            padding_length = max_length - len(input_ids)
            if pad_on_left:
                input_ids = ([pad_token] * padding_length) + input_ids
                attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask
                token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids
            else:
                input_ids = input_ids + ([pad_token] * padding_length)
                attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)
                token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)

            assert len(input_ids) == max_length
            assert len(attention_mask) == max_length
            assert len(token_type_ids) == max_length
            choices_features.append((input_ids, attention_mask, token_type_ids))
        label = label_map[example.label]

        if log and ex_index < 2:
            logger.info("*** Example ***")
            logger.info("race_id: {}".format(example.example_id))
            for choice_idx, (input_ids, attention_mask, token_type_ids) in enumerate(choices_features):
                logger.info("choice: {}".format(choice_idx))
                logger.info("input_ids: {}".format(" ".join(map(str, input_ids))))
                logger.info("attention_mask: {}".format(" ".join(map(str, attention_mask))))
                logger.info("token_type_ids: {}".format(" ".join(map(str, token_type_ids))))
                logger.info("label: {}".format(label))

        features.append(InputFeatures(example_id=example.example_id, choices_features=choices_features, label=label, ))

    return features


def select_field(features, field):
    return [
        [choice[field] for choice in feature.choices_features]
        for feature in features
    ]


def prepare_for_qa(data, tokenizer, evaluate=False, test=False):
    assert not (evaluate and test)

    label_list = processor.get_labels()

    examples = [

    ]

    features = convert_examples_to_features(
        examples,
        label_list,
        args.max_seq_length,
        tokenizer,
        pad_on_left=False,
        pad_token_segment_id=0,
    )
    all_input_ids = torch.tensor(select_field(features, "input_ids"), dtype=torch.long)
    all_input_mask = torch.tensor(select_field(features, "input_mask"), dtype=torch.long)
    all_segment_ids = torch.tensor(select_field(features, "segment_ids"), dtype=torch.long)
    all_label_ids = torch.tensor([f.label for f in features], dtype=torch.long)

    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)
    return dataset


"""#### my

from transformers import BertForMultipleChoice, BertConfig


config = BertConfig.from_pretrained(
    f'{PATH}/output/config.json',
    num_labels=4,
    finetuning_task='race',
    cache_dir=f'{PATH}/cache',
)

qa_model = BertForMultipleChoice.from_pretrained(
    f'{PATH}/output/pytorch_model.bin',
    config=config,
    cache_dir=f'{PATH}/cache',
)
"""

# input_example = InputExample(example_id=-1, question, contexts, endings, label=None)

# input_features = InputFeatures(example_id, choices_features, label)

from transformers import RobertaTokenizer
from transformers import RobertaForMultipleChoice

"""ds_parameter = {'add_tokens': 0,
 'batch': 10,
 'cache': False,
 'config': 'bert-base-cased',
 'epoch': 6,
 'grad_accum': 1,
 'lazy': True,
 'likelihood': 'none',
 'lr': [5e-05],
 'maxlen': 512,
 'model': ['onebyone'],
 'panel': False,
 'savedir': '/drive/My Drive/Thesis/BDG/race_cqa_gen_d/',
 'seed': 609,
 'setting': 'QA_pretrained_triplet',
 'tensorboard': True,
 'test': ['/drive/My Drive/Thesis/BDG/processed_data/race_test_updated_cqa_dsep_a.csv'],
 'train': ['/drive/My Drive/Thesis/BDG/processed_data/race_dev_updated_cqa_dsep_a.csv'],
 'worker': 8}

train_ds = get_dataset(f'{PATH}/processed_data/race_dev_updated_cqa_dsep_a.csv', model_class=None, parameter=ds_parameter, cut=100, lazy=False)
train_dataset = [train_ds]

train_dataset = [data.DataLoader(dataset=ds,
                batch_size=16,
                shuffle=True,
                num_workers=8) for ds in train_dataset]
                """

from transformers import BertForMultipleChoice
import torch
from transformers import RobertaTokenizer
from transformers import RobertaForMultipleChoice


class Model_QA_pretrained_triplet(Model):
    """
    Three options are available:
    - correct answer
    - gold distractor
    - generated distractor
    """

    def __init__(self, tokenizer, pretrained, maxlen=512, **kwargs):
        qa_model_path = kwargs.pop('qa_model', '')
        super().__init__(tokenizer, pretrained, maxlen, **kwargs)
        print('qa_model_path', qa_model_path, bool(qa_model_path))
        if qa_model_path:
            raise NotImplementedError('Not implemented')
            # self.qa_tokenizer = self.tokenizer # shouldn't tokenizer be loaded as well?
            # self.qa_model = BertForMultipleChoice.from_pretrained('roberta-base-openai-detector')
            # self.qa_model.load_state_dict(torch.load(qa_model_path))
        else:
            # pass
            qa_tokenizer = RobertaTokenizer.from_pretrained(
                "LIAMF-USP/roberta-large-finetuned-race")
            qa_model = RobertaForMultipleChoice.from_pretrained(
                "LIAMF-USP/roberta-large-finetuned-race")
            self.qa_model = qa_model
            self.qa_tokenizer = qa_tokenizer

        self.qa_model.eval()
        self.qa_model.to(self.device)

        self.predict_parameter = load_predict_parameter(self, False)
        self.predict_parameter.pop('input')
        if 'task' not in self.predict_parameter:
            self.predict_parameter.update({'task': 'default'})

    def qa(self, context, question, options, label_example):
        choices_inputs = []
        for ending_idx, (_, ending) in enumerate(
                zip(context, options)):

            if question.find("_") != -1:  # fill in the blanks questions
                question_option = question.replace("_", ending)
            else:
                question_option = question + " " + ending
            inputs = self.qa_tokenizer(
                context,
                question_option,
                add_special_tokens=True,
                max_length=512,
                padding="max_length",
                truncation=True,
                return_overflowing_tokens=False,
            )
            choices_inputs.append(inputs)

        label = torch.LongTensor([label_map[label_example]])
        input_ids = torch.LongTensor([
            [x["input_ids"] for x in choices_inputs]
        ])
        attention_mask = (
            torch.Tensor([[x["attention_mask"] for x in choices_inputs]])
            # as the senteces follow the same structure, just one of them is necessary to check
            if "attention_mask" in choices_inputs[0]
            else None
        )

        example_encoded = {
            # "example_id": example_id,
            "input_ids": input_ids.to(self.device),
            "attention_mask": attention_mask.to(self.device),
            "labels": label.to(self.device),
        }

        output = self.qa_model(**example_encoded)
        return output

    def get_qa_loss(self, inputs, starts):
        losses = []
        for i in range(len(inputs)):
            result, result_dict = self.predict(
                tokenizer.convert_tokens_to_string(
                    tokenizer.convert_ids_to_tokens(
                        inputs[i]
                    )
                ),
                **self.predict_parameter
            )
            gen_distractor = result[0]

            # print(tokenizer.convert_ids_to_tokens(inputs[i]))
            cqa = tokenizer.convert_ids_to_tokens(inputs[i])[1:starts[i]]
            cqa = tokenizer.convert_tokens_to_string(cqa)
            # print(cqa.split('[SEP]'), len( cqa.split('[SEP]') ))
            print(cqa)
            context, question, answer, true_distractor = cqa.split('[SEP]')

            out = self.qa(context, question, [answer, true_distractor, gen_distractor], "A")
            losses.append(out.loss)

        return torch.Tensor(losses).mean()

    def forward(self, batch_data, eval=False):
        inputs = batch_data['input']
        targets = batch_data['target']
        negative_targets = batch_data['ntarget']
        masks = batch_data['mask']
        starts = batch_data['start']

        # torch.save(batch_data, 'batch.pt')
        # for name, var in [['inputs', inputs], ['negative_targets', negative_targets], ['targets', targets], ]:
        #    print(name, var.shape, var, np.sum(var))
        #    #print(tokenizer.convert_ids_to_tokens(var[0]), '\n')

        tokens_tensor = torch.as_tensor(inputs).to(self.device)
        mask_tensors = torch.as_tensor(masks).to(self.device)

        # print(tokens_tensor.device, mask_tensors.device)
        # print('self.pretr', self.pretrained.device)
        # print('self.model', None) #, self.model.device)

        outputs = self.pretrained(tokens_tensor, attention_mask=mask_tensors)
        prediction_scores = self.model(outputs[0])  # == self.model(outputs.last_hidden_state)

        if eval:
            result_dict = {
                'label_prob_all': [],
                'label_map': [],
                'prob_list': []
            }
            start = batch_data['start'][0]
            topK = torch.topk(softmax(prediction_scores[0][start], dim=0), 50)
            logit_prob = softmax(prediction_scores[0][start], dim=0).data.tolist()
            prob_result = [(self.tokenizer.convert_ids_to_tokens(id), prob) for prob, id in
                           zip(topK.values.data.tolist(), topK.indices.data.tolist())]
            result_dict['prob_list'].append(logit_prob)
            result_dict['label_prob_all'].append(prob_result)
            result_dict['label_map'].append(prob_result[0])
            outputs = result_dict
            """Try this
            start = batch_data['start'][0]
            logit_prob = softmax(prediction_scores[0][start], dim=0).data.tolist()
            prob_result = {self.tokenizer.convert_ids_to_tokens(id): prob for id, prob in enumerate(logit_prob)}
            prob_result = sorted(prob_result.items(), key=lambda x: x[1], reverse=True)
            result_dict['prob_list'].append(sorted(logit_prob, reverse=True))
            """
        else:  # if train
            loss_tensors = torch.as_tensor(targets).to(self.device)
            negativeloss_tensors = torch.as_tensor(negative_targets).to(self.device)

            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)  # -1 index = padding token
            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.pretrained.config.vocab_size),
                                      loss_tensors.view(-1))

            negative_loss_fct = NegativeCElLoss(ignore_index=-1).to(self.device)
            negative_loss = negative_loss_fct(prediction_scores.view(-1, self.pretrained.config.vocab_size),
                                              negativeloss_tensors.view(-1))

            qa_loss = self.get_qa_loss(inputs, starts)

            # masked_lm_loss += negative_loss
            outputs = masked_lm_loss + negative_loss + qa_loss

        return outputs


'''model = Model_QA_pretrained_triplet(tokenizer=tokenizer, pretrained=pretrained, tasks_detail={'default': []},
                                  maxlen=input_arg.get('maxlen'))
model = model.to(device)
batch = torch.load('batch.pt')

model(batch)'''

_ = """dataloader = train_dataset[0]
batch = next(iter(dataloader))
#print(batch)
model = Model_QA_pretrained_triplet(tokenizer=AutoTokenizer.from_pretrained('bert-base-multilingual-cased'), 
                                    pretrained=AutoModel.from_pretrained('bert-base-multilingual-cased'), 
                                    maxlen=512)
print('init finished')
predict_parameter = load_predict_parameter(model, enable_arg_panel=False)
model(batch)
"""

# print(1/0)

MODEL_CONFIG_MAPPING = {
    'standard': Model,
    'QA_pretrained_triplet': Model_QA_pretrained_triplet
}

"""## Train

### Functions
"""

import argparse
import sys
import shlex

import nlp2
# import src
import torch
from tqdm import tqdm
from transformers import BertTokenizer, AutoTokenizer, AutoModel
from torch.utils import data
from itertools import zip_longest
from datetime import datetime
import os

# import src.utility.tok as tok
# from src.utility.dataset import get_dataset
# from src.utility.logger import Logger
# from src.utility.model_loader import load_model_class
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"


def parse_train_args(args):
    print(args)
    parser = argparse.ArgumentParser()
    exceed_mode = nlp2.function_get_all_arg_with_value(handle_exceed)['mode']  # possible exceed_modes
    parser.add_argument("--batch", type=int, default=20, help="batch size, default 20")
    parser.add_argument("--lr", type=float, nargs='+', default=[5e-5], help="learning rate, default 5e-5")
    parser.add_argument("--epoch", type=int, default=10, help="epoch, default 10")
    parser.add_argument("--maxlen", type=int, default=512, help="max tokenized sequence length, default 512")
    parser.add_argument("--handle_exceed", choices=exceed_mode,
                        help='select ways to handle input exceed max length')
    parser.add_argument("--savedir", type=str, default="checkpoints/", help="model saving dir, default /checkpoints")
    parser.add_argument("--add_tokens", type=int, default=0,
                        help="auto add freq > x UNK token to word table")
    parser.add_argument("--train", type=str, nargs='+', required=True, help="train dataset path")
    parser.add_argument("--test", type=str, nargs='+', required=True, help="test dataset path")
    parser.add_argument("--model", type=str, required=True, nargs='+',
                        choices=list_all_model(), help="model task")
    parser.add_argument("--tag", type=str, nargs='+', help="tag to identity task in multi-task")
    parser.add_argument("--config", type=str, default='bert-base-multilingual-cased', required=True,
                        help='distilbert-base-multilingual-cased|voidful/albert_chinese_small')
    parser.add_argument("--seed", type=int, default=609, help="random seed, default 609")
    parser.add_argument("--worker", type=int, default=8, help="number of worker on pre-processing, default 8")
    parser.add_argument("--grad_accum", type=int, default=1, help="gradient accumulation, default 1")
    parser.add_argument('--tensorboard', dest='tensorboard', action='store_true', help='Turn on tensorboard graphing')
    parser.add_argument("--resume", help='resume training')
    # parser.add_argument("--cache", action='store_true', help='cache training data')
    parser.add_argument("--cache", type=bool, default=False, help='cache training data')
    parser.add_argument("--panel", action='store_true', help="enable panel to input argument")
    parser.add_argument("--likelihood", type=str, default='none', choices=['none', 'pos', 'neg', 'both'],
                        help='likelihood')
    parser.add_argument("--lazy", type=bool, default=False)
    parser.add_argument("--setting", type=str, choices=list(MODEL_CONFIG_MAPPING.keys()), required=True,
                        default='standard')
    parser.add_argument("--qa_model", type=str, required=False)

    input_arg, model_arg = parser.parse_known_args(args)
    print(input_arg)
    print(model_arg)
    input_arg = {k: v for k, v in vars(input_arg).items() if v is not None}
    model_arg = {k.replace("--", ""): v for k, v in zip(model_arg[:-1:2], model_arg[1::2])}  # make it dict

    return input_arg, model_arg


def optimizer(model, lr):
    return torch.optim.AdamW(model.parameters(), lr=lr)


def load_model_and_datas(tokenizer, pretrained, device, model_arg, input_arg, cut=None, lazy=False):
    models = []
    train_dataset = []
    test_dataset = []
    train_ds_maxlen = 0
    test_ds_maxlen = 0
    for model_class_name, train_file, test_file in zip_longest(input_arg.get('model'),
                                                               input_arg.get('train'),
                                                               input_arg.get('test'),
                                                               fillvalue=""):  # zip_longest('ABCD', 'xy', fillvalue='-') --> Ax By C- D-
        # get model class
        model_class = get_model_class()

        # load dataset
        ds_parameter = {**model_arg, **input_arg}
        train_ds = get_dataset(train_file, model_class, ds_parameter, cut=cut, lazy=lazy)
        test_ds = get_dataset(test_file, model_class, ds_parameter, cut=cut, lazy=lazy)

        # load model
        model = model_class(tokenizer=tokenizer, pretrained=pretrained, tasks_detail=train_ds.task,
                            maxlen=input_arg.get('maxlen'))
        model = model.to(device)

        # append to max len
        train_ds_maxlen = train_ds.__len__() if train_ds.__len__() > train_ds_maxlen else train_ds_maxlen
        test_ds_maxlen = test_ds.__len__() if test_ds.__len__() > test_ds_maxlen else test_ds_maxlen

        train_dataset.append(train_ds)
        test_dataset.append(test_ds)
        models.append(model)

    return models, train_dataset, test_dataset, train_ds_maxlen, test_ds_maxlen


def save_model(models, input_arg, models_tag, epoch, fname, logger):
    save_model = {
        'models': [m.state_dict() for m in models],
        'model_config': input_arg.get('config'),
        'tags': models_tag,
        'type': input_arg.get('model'),
        'maxlen': input_arg.get('maxlen'),
        'epoch': epoch
    }

    for ind, m in enumerate(input_arg.get('model')):
        if 'tag' in m:
            save_model['label'] = models[ind].labels
        if "clas" in m:
            save_model['task-label'] = models[ind].tasks_detail

    torch.save(save_model, f"{fname}.pt")
    logger.write_log(f"weights were saved to {fname}.pt")


def model_eval(models, test_dataset, fname, input_arg, epoch, logger):
    t_loss = 0
    t_length = 0
    for m in models:
        m.eval()

    with torch.no_grad():
        iters = [iter(ds) for ds in test_dataset]
        end = False
        total_iter_length = len(iters[0])
        pbar = tqdm(total=total_iter_length, desc='eval')
        while not end:
            for model, batch in zip(models, iters):
                test_batch = next(batch, None)
                if test_batch is not None:
                    loss = model(test_batch)
                    loss = loss / input_arg.get('grad_accum')
                    t_loss += loss.mean().item()
                    t_length += 1
                    pbar.update(1)
                else:
                    end = True
        pbar.close()

    avg_t_loss = t_loss / t_length if t_length > 0 else 0
    logger.write_log(f"model: {fname}, Total Loss: {avg_t_loss}")
    logger.write_metric("eval_loss/step", avg_t_loss, epoch)
    return avg_t_loss


def model_train(models_list, train_dataset, models_tag, input_arg, epoch, logger,
                use_qa=False):
    optims = []
    models = []
    for i, m in enumerate(models_list):  # usually there is only one model
        model = torch.nn.DataParallel(m)  # Data parallelism at the module level.
        model.train()  # turn on train mode
        models.append(model)
        optims.append(optimizer(
            m, input_arg.get('lr')[i] if i < len(input_arg.get('lr'))  # user lr at position `i` if there is such one
            else input_arg.get('lr')[0]
        ))

    total_iter = 0
    t_loss = 0  # total loss

    iters = [iter(ds) for ds in train_dataset]
    total_iter_length = len(iters[0])
    end = False

    pbar = tqdm(total=total_iter_length, desc='train', leave=True, position=0)
    while not end:
        for i, (model, optim, mtag, batch) in enumerate(zip(models, optims, models_tag, iters)):
            # print(i)
            train_batch = next(batch, None)
            if train_batch is not None:
                loss = model(train_batch)  # process a train_batch through the model (forward) and get loss
                loss = loss / input_arg.get('grad_accum')
                loss.mean().backward()  # take a mean (if non-scalar is returned) and do the backpropagation
                if (total_iter + 1) % input_arg.get(
                        'grad_accum') == 0:  # make updates only after performing `grad_accum` number of steps
                    optim.step()  # make a single optimization step
                    optim.zero_grad()  # zero the parameter gradients
                    model.zero_grad()
                t_loss += loss.mean().item()
                logger.write_metric("loss/step", loss.mean().item(), epoch)
                if total_iter % 100 == 0 and total_iter != 0:  # monitoring
                    logger.write_log(
                        f"epoch: {epoch}, tag: {mtag}, model: {model.module.__class__.__name__}, step: {total_iter}, loss: {t_loss / total_iter if total_iter > 0 else 0}, total:{total_iter_length}")
            else:
                end = True
        pbar.update(1)
        total_iter += 1
    pbar.close()
    logger.write_log(
        f"epoch: {epoch}, step: {total_iter}, loss: {t_loss / total_iter if total_iter > 0 else 0}, total: {total_iter}")
    return t_loss / total_iter


"""### Run"""

# TODO change file to `train` in --train
# TODO change model to `onebyone`
arg = shlex.split(
    f"--maxlen 512 " \
    f"--savedir '{PATH}/race_cqa_gen_d/' " \
    f"--train '{DATA_PATH}/race_train_updated_cqa_dsep_a.csv' " \
    f"--test '{DATA_PATH}/race_test_updated_cqa_dsep_a.csv' " \
    f"--model onebyone --likelihood none " \
    f"--tensorboard  --config bert-base-cased --batch 8 --epoch 6 " \
    f"--cache True " \
    #f"--lazy False " \
    f"--setting standard " \
    f"--worker 8 "
)

input_arg, model_arg = parse_train_args(sys.argv[1:]) if arg is None else parse_train_args(arg)
nlp2.get_dir_with_notexist_create(input_arg.get('savedir'))  # it will create a new dir if not exist
logger = Logger(savedir=input_arg.get('savedir'), tensorboard=input_arg.get('tensorboard'))
device = 'cuda' if torch.cuda.is_available() else 'cpu'
nlp2.set_seed(input_arg.get('seed'))  # set seeds for random, numpy, torch and torch.cuda, make torch reproducible

logger.write_log(datetime.now().__str__())
logger.write_log("TRAIN PARAMETER")
logger.write_log("=======================")
[logger.write_log(str(key) + " : " + str(value)) for key, value in input_arg.items()]
logger.write_log("=======================")

# load pre-train model
pretrained_config = input_arg.get('config')
if 'albert_chinese' in pretrained_config:
    tokenizer = BertTokenizer.from_pretrained(pretrained_config)
else:
    tokenizer = AutoTokenizer.from_pretrained(pretrained_config)
pretrained = AutoModel.from_pretrained(pretrained_config)

# handling add tokens
if input_arg.get('add_tokens'):  # add freq > input_arg.add_tokens UNK token to word table
    logger.write_log("Calculating Unknown Token")
    add_tokens = get_freqK_unk_token(tokenizer, input_arg.get('train') + input_arg.get('test'),
                                     input_arg.get('add_tokens'))
    num_added_toks = tokenizer.add_tokens(add_tokens)
    logger.write_log('We have added', num_added_toks, 'tokens')
    pretrained.resize_token_embeddings(len(tokenizer))
    save_path = os.path.join(input_arg.get('savedir'), pretrained_config + "_added_tok")
    pretrained.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    logger.write_log('New pre-train model saved at ', save_path)
    logger.write_log("=======================")

# load model and data
models_tag = (
    input_arg.get('tag') if input_arg.get('tag', None) is not None
    else [m.lower() + "_" + str(ind) for ind, m in enumerate(input_arg.get('model'))]
)
print(f'models_tag = {models_tag}')

lazy = input_arg.get('lazy')
print('lazy', lazy)
cut = None  # int(1) # TODO !!! set to None if you have enough memory
if cut is not None:
    print(f'Attention! `cut` is set to {cut}')
if 'train' not in input_arg.get('train')[0]:
    print('Attention! Check the train arg')
models, train_dataset, test_dataset, train_ds_maxlen, test_ds_maxlen = load_model_and_datas(tokenizer,
                                                                                            pretrained,
                                                                                            device,
                                                                                            model_arg,
                                                                                            input_arg,
                                                                                            cut=cut,
                                                                                            lazy=lazy)
"""# balance sample for multi-task
for ds in train_dataset:
    ds.increase_with_sampling(train_ds_maxlen)
for ds in test_dataset:
    ds.increase_with_sampling(test_ds_maxlen)"""

# print(type(train_dataset[0]), type(ds))


train_dataset = [data.DataLoader(dataset=ds,
                                 batch_size=input_arg.get('batch'),
                                 shuffle=True,
                                 num_workers=input_arg.get('worker')) for ds in train_dataset]
# create a DataLoader for each Dataset in the list
test_dataset = [data.DataLoader(dataset=ds,
                                batch_size=input_arg.get('batch'),
                                shuffle=True,
                                num_workers=input_arg.get('worker')) for ds in test_dataset]

# loading model back
start_epoch = 1
if input_arg.get('resume'):  # resume training
    logger.write_log("Loading back:", input_arg.get('resume'))
    package = torch.load(input_arg.get('resume'), map_location=device)  # map_location - how to remap storage locations
    if 'model_state_dict' in package:
        models[0].load_state_dict(package['model_state_dict'])
    else:
        for model_tag, state_dict in zip(package['tags'], package['models']):
            tag_ind = package['tags'].index(model_tag)
            models[tag_ind].load_state_dict(state_dict)
    start_epoch = int(package.get('epoch', 1)) + 1
print('start_epoch', start_epoch)
"""### Cycle"""

# train/eval loop

logger.write_log("training batch : " + str(input_arg.get('batch') * input_arg.get('grad_accum')))
for epoch in tqdm(range(start_epoch, start_epoch + input_arg.get('epoch')), desc='train epoch'):
    fname = os.path.join(input_arg.get('savedir'), str(epoch))

    logger.write_log(f"=========train at epoch={epoch}=========")
    try:
        train_avg_loss = model_train(models, train_dataset, models_tag, input_arg, epoch, logger)
    except KeyboardInterrupt:
        save_model(models, input_arg, models_tag, epoch, fname + "_interrupt", logger)
        pass

    logger.write_log(f"=========save at epoch={epoch}=========")
    save_model(models, input_arg, models_tag, epoch, fname, logger)

    logger.write_log(f"=========eval at epoch={epoch}=========")
    eval_avg_loss = model_eval(models, test_dataset, fname, input_arg, epoch, logger)

    logger.write_metric("train_loss/epoch", train_avg_loss, epoch)
    logger.write_metric("eval_loss/epoch", eval_avg_loss, epoch)

"""## Eval"""

# !pip install git+https://github.com/voidful/nlg-eval.git ; nlg-eval --setup ./nlg-eval-data/

import argparse
import sys

from tqdm import tqdm
import csv
import shlex


# from src.utility.eval_metric import EvalMetric
# from src.utility.model_loader import load_trained_model, load_predict_parameter


def parse_eval_args(args):
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", required=True, type=str, help="model path")
    parser.add_argument("--config", type=str, help='pre-trained model path after add token')
    parser.add_argument("--metric", required=True, type=str, choices=['emf1', 'nlg', 'clas'], help="evaluate metric")
    parser.add_argument("--valid", required=True, type=str, nargs='+', help="evaluate data path")
    parser.add_argument("--print", action='store_true', help="print each pair of evaluate data")
    parser.add_argument("--panel", action='store_true', help="enable panel to input argument")
    parser.add_argument("--setting", type=str, choices=list(MODEL_CONFIG_MAPPING.keys()), required=True,
                        default='standard')
    return vars(parser.parse_args(args))


model_path = f"'{PATH}/race_cqa_gen_d/2.pt'"
print(model_path)
arg = shlex.split(
    f"""
    --model {model_path}
    --valid '{DATA_PATH}/race_test_updated_cqa_dall.csv'
    --metric nlg
    --print 
    --setting standard
    """
)

"""
"""

eval_arg = parse_eval_args(sys.argv[1:]) if arg is None else parse_eval_args(arg)
print(eval_arg)

valid = eval_arg.get('valid')[0]
model, model_type, model_class = load_trained_model(eval_arg.get('model'), pretrained_config=eval_arg.get('config'))
eval_dataset = get_data_from_file(valid)
predict_parameter = load_predict_parameter(model, eval_arg.get('panel'))

if 'decodenum' in predict_parameter and predict_parameter['decodenum'] > 1:
    eval_metrics = [EvalMetric(model.tokenizer) for _ in range(predict_parameter['decodenum'])]
else:
    eval_metrics = [EvalMetric(model.tokenizer)]

print("PREDICT PARAMETER")
print("=======================")
print(predict_parameter)
print("=======================")
for i in tqdm(eval_dataset):
    tasks = i[0]
    task = i[1]
    input = i[2]
    target = i[3]

    predict_parameter.update({'input': input})
    if 'task' not in predict_parameter:
        predict_parameter.update({'task': task})
    result, result_dict = model.predict(**predict_parameter)
    print(result, result_dict)

    for eval_pos, eval_metric in enumerate(eval_metrics):
        # predicted can be list of string or string
        # target should be list of string
        predicted = result
        processed_target = target
        if 'qa' in model_type:
            processed_target = " ".join(input.split(" ")[int(target[0]): int(target[1])])
            if len(result) > 0:
                predicted = result[0][0] if isinstance(result[0], list) else result[0]
            else:
                predicted = ''
        elif 'onebyone' in model_type or 'seq2seq' in model_type:  # this one
            processed_target = " ".join(target[0])
            if len(result) < eval_pos:
                print("Decode size smaller than decode num:", result_dict['label_map'])
            predicted = result[eval_pos]
        elif 'once' in model_type:
            processed_target = target[0]
            predicted = result[eval_pos]
        elif 'mask' in model_type:
            processed_target = target[0].split(" ")
            predicted = result
        elif 'tag' in model_type:
            predicted = " ".join([list(d.values())[0] for d in result_dict[0]['label_map']])
            processed_target = target[0].split(" ")
            predicted = predicted.split(" ")

        if eval_arg.get('print'):
            print('===eval===')
            print("input: ", input)
            print("target: ", processed_target)
            print("predicted: ", predicted)
            print('==========')

        eval_metric.add_record(input, predicted, processed_target)

for eval_pos, eval_metric in enumerate(eval_metrics):
    argtype = "_dataset" + valid.replace("/", "_").replace(".", "")
    if 'decodenum' in predict_parameter and predict_parameter['decodenum'] > 1:
        argtype += "_num_" + str(eval_pos)
    if 'mode' in predict_parameter:
        para_mode = predict_parameter['mode'][0] if isinstance(predict_parameter['mode'], list) else \
            predict_parameter['mode'].lower()
        argtype += "_mode_" + str(para_mode)
    if 'filtersim' in predict_parameter:
        argtype += "_filtersim_" + str(predict_parameter['filtersim'])
    outfile_name = eval_arg.get('model') + argtype

    with open(outfile_name + "_predicted.csv", "w", encoding='utf8') as f:
        writer = csv.writer(f)
        records = eval_metric.get_record()
        writer.writerow(['input', 'predicted', 'targets'])
        for i, p, t in zip(records['input'], records['predicted'], records['targets']):
            writer.writerow([i, p, "[SEP]".join([onet for onet in t if len(onet) > 0])])
    print("write result at:", outfile_name)

    with open(outfile_name + "_each_data_score.csv", "w", encoding='utf8') as edsf:
        eds = csv.writer(edsf)
        with open(outfile_name + "_score.csv", "w", encoding='utf8') as f:
            for i in eval_metric.cal_score(eval_arg.get('metric')):
                f.write("TASK: " + str(i[0]) + " , " + str(eval_pos) + '\n')
                f.write(str(i[1]) + '\n')
                eds.writerows(i[2])

    print("write score at:", outfile_name)

    for i in eval_metric.cal_score(eval_arg.get('metric')):
        print("TASK: ", i[0], eval_pos)
        print(i[1])
